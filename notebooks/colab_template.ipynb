{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1d24bf",
   "metadata": {},
   "source": [
    "# Modelo KNN para intencion de voto\n",
    "Sube `voter_intentions_3000.csv` (Files > Upload) y ejecuta las celdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas==2.2.2 scikit-learn==1.4.2 matplotlib==3.8.4 joblib==1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f84bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    Normalizer,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49632c51",
   "metadata": {},
   "source": [
    "## 1. Cargar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acfd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "DATA_PATH = \"voter_intentions_3000.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c8464",
   "metadata": {},
   "source": [
    "## 2. Definir transformadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "@dataclass\n",
    "class SecondaryChoiceImputer(BaseEstimator, TransformerMixin):\n",
    "    numeric_features: Iterable[str]\n",
    "    primary_feature: str = \"primary_choice\"\n",
    "    target_feature: str = \"secondary_choice\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._classifier = RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=12,\n",
    "            random_state=42,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "        )\n",
    "        self._primary_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        self._numeric_medians: Optional[pd.Series] = None\n",
    "        self._should_skip = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        frame = X.copy()\n",
    "        mask = frame[self.target_feature].notna()\n",
    "        if mask.sum() == 0:\n",
    "            self._should_skip = True\n",
    "            return self\n",
    "        working = frame.loc[mask, list(self.numeric_features) + [self.primary_feature, self.target_feature]]\n",
    "        self._numeric_medians = working[self.numeric_features].median()\n",
    "        X_num = working[self.numeric_features].fillna(self._numeric_medians).to_numpy()\n",
    "        X_cat = self._primary_encoder.fit_transform(working[[self.primary_feature]])\n",
    "        X_model = np.hstack([X_num, X_cat])\n",
    "        y_model = working[self.target_feature]\n",
    "        self._classifier.fit(X_model, y_model)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        frame = X.copy()\n",
    "        if self._should_skip:\n",
    "            frame[self.target_feature] = frame[self.target_feature].fillna(\"Unknown\")\n",
    "            return frame\n",
    "        mask = frame[self.target_feature].isna()\n",
    "        if mask.sum() == 0:\n",
    "            return frame\n",
    "        medians = self._numeric_medians if self._numeric_medians is not None else frame[self.numeric_features].median()\n",
    "        X_num = frame.loc[mask, self.numeric_features].fillna(medians).to_numpy()\n",
    "        X_cat = self._primary_encoder.transform(frame.loc[mask, [self.primary_feature]])\n",
    "        preds = self._classifier.predict(np.hstack([X_num, X_cat]))\n",
    "        frame.loc[mask, self.target_feature] = preds\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bd8e7",
   "metadata": {},
   "source": [
    "## 3. Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a27aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"intended_vote\"\n",
    "ALL_COLS = df.columns.drop([TARGET])\n",
    "NUMERIC_FEATURES = df[ALL_COLS].select_dtypes(include=[np.number]).columns.tolist()\n",
    "CAT_FEATURES = [c for c in ALL_COLS if c not in NUMERIC_FEATURES]\n",
    "\n",
    "# Garantizamos que las columnas categoricas clave esten en la lista correcta\n",
    "for col in [\"primary_choice\", \"secondary_choice\"]:\n",
    "    if col in NUMERIC_FEATURES:\n",
    "        NUMERIC_FEATURES.remove(col)\n",
    "    if col not in CAT_FEATURES:\n",
    "        CAT_FEATURES.append(col)\n",
    "\n",
    "print(\"Num:\", len(NUMERIC_FEATURES), NUMERIC_FEATURES[:5], \"...\")\n",
    "print(\"Cat:\", len(CAT_FEATURES), CAT_FEATURES[:5], \"...\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df[TARGET])\n",
    "X = df[NUMERIC_FEATURES + CAT_FEATURES]\n",
    "y = y_encoded\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "X_train = X_train.reset_index(drop=False).rename(columns={\"index\": \"source_id\"})\n",
    "y_train = pd.Series(y_train).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fbd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Balanceo de clases SOLO en entrenamiento con grupos ===\n",
    "train_df = X_train.copy()\n",
    "train_df[\"__y__\"] = y_train.values\n",
    "\n",
    "counts = train_df[\"__y__\"].value_counts()\n",
    "mayor = counts.idxmax()\n",
    "n_mayor = counts.max()\n",
    "\n",
    "cap_mayor = int(n_mayor * 0.4)\n",
    "target = cap_mayor\n",
    "\n",
    "frames = []\n",
    "rng = 42\n",
    "for cls, cnt in counts.items():\n",
    "    dfc = train_df[train_df[\"__y__\"] == cls]\n",
    "    if cls == mayor:\n",
    "        frames.append(dfc.sample(n=cap_mayor, random_state=rng))\n",
    "    else:\n",
    "        frames.append(dfc.sample(n=target, replace=True, random_state=rng))\n",
    "\n",
    "train_bal = pd.concat(frames, ignore_index=True).sample(frac=1.0, random_state=rng)\n",
    "\n",
    "X_train_bal = train_bal.drop(columns=[\"__y__\"])\n",
    "y_train_bal = train_bal[\"__y__\"].to_numpy()\n",
    "groups = X_train_bal[\"source_id\"].to_numpy()\n",
    "X_train_bal = X_train_bal.drop(columns=[\"source_id\"])\n",
    "\n",
    "print(\"Distribucion balanceada:\\n\", pd.Series(y_train_bal).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", IterativeImputer(estimator=LinearRegression(), max_iter=10, random_state=42)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, NUMERIC_FEATURES),\n",
    "    (\"cat\", cat_transformer, CAT_FEATURES),\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"secondary_choice_imputer\", SecondaryChoiceImputer(NUMERIC_FEATURES)),\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"row_norm\", \"passthrough\"),\n",
    "    (\"dimreduce\", \"passthrough\"),\n",
    "    (\"knn\", KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"row_norm\": [\"passthrough\"],\n",
    "        \"dimreduce\": [\"passthrough\", PCA(n_components=50)],\n",
    "        \"knn__metric\": [\"minkowski\"],\n",
    "        \"knn__p\": [1, 2],\n",
    "        \"knn__n_neighbors\": [5, 15, 35],\n",
    "        \"knn__weights\": [\"distance\"],\n",
    "    },\n",
    "    {\n",
    "        \"row_norm\": [Normalizer()],\n",
    "        \"dimreduce\": [\"passthrough\", PCA(n_components=50)],\n",
    "        \"knn__metric\": [\"cosine\"],\n",
    "        \"knn__n_neighbors\": [5, 15, 35],\n",
    "        \"knn__weights\": [\"distance\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "scoring = {\n",
    "    \"macro_f1\": \"f1_macro\",\n",
    "    \"bal_acc\": \"balanced_accuracy\",\n",
    "}\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=gkf,\n",
    "    scoring=scoring,\n",
    "    refit=\"macro_f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "search.fit(X_train_bal, y_train_bal, groups=groups)\n",
    "print(\"Mejores hiperparametros:\", search.best_params_)\n",
    "print(\"Mejor macro_f1 (cv):\", search.best_score_)\n",
    "best_model = search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2858cad",
   "metadata": {},
   "source": [
    "## 4. Evaluacion y artefactos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca02995",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "print(classification_report(y_test_labels, y_pred_labels, zero_division=0))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_labels, y_pred_labels, xticks_rotation=45)\n",
    "plt.title(\"Matriz de confusion (todas las clases)\")\n",
    "plt.show()\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"Macro F1: {macro_f1:.3f} | Balanced Acc: {bal_acc:.3f}\")\n",
    "\n",
    "try:\n",
    "    undec_idx = np.where(label_encoder.classes_ == \"Undecided\")[0][0]\n",
    "    mask_decididos = y_test != undec_idx\n",
    "    if mask_decididos.any():\n",
    "        y_test_dec = y_test[mask_decididos]\n",
    "        y_pred_dec = y_pred[mask_decididos]\n",
    "        y_test_dec_lab = label_encoder.inverse_transform(y_test_dec)\n",
    "        y_pred_dec_lab = label_encoder.inverse_transform(y_pred_dec)\n",
    "\n",
    "        print(\"\\n=== SOLO DECIDIDOS (excluye 'Undecided') ===\")\n",
    "        print(classification_report(y_test_dec_lab, y_pred_dec_lab, zero_division=0))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test_dec_lab, y_pred_dec_lab, xticks_rotation=45)\n",
    "        plt.title(\"Matriz de confusion (solo decididos)\")\n",
    "        plt.show()\n",
    "\n",
    "        macro_f1_dec = f1_score(y_test_dec, y_pred_dec, average=\"macro\", zero_division=0)\n",
    "        bal_acc_dec = balanced_accuracy_score(y_test_dec, y_pred_dec)\n",
    "        print(f\"Macro F1 (decididos): {macro_f1_dec:.3f} | Balanced Acc (decididos): {bal_acc_dec:.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"Aviso decididos:\", e)\n",
    "\n",
    "joblib.dump({\"model\": best_model, \"label_encoder\": label_encoder}, \"knn_voter_intentions.joblib\")\n",
    "print(\"Modelo guardado en knn_voter_intentions.joblib\")\n",
    "files.download(\"knn_voter_intentions.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c83ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluacion post-proceso: Umbral de 'Undecided' ===\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "proba = best_model.predict_proba(X_test)\n",
    "classes = label_encoder.classes_\n",
    "idx_u = list(classes).index(\"Undecided\")\n",
    "\n",
    "def eval_tau(tau):\n",
    "    top = proba.argmax(1)\n",
    "    use_u = proba[:, idx_u] > tau\n",
    "    y_pred_tau = top.copy()\n",
    "    y_pred_tau[use_u] = idx_u\n",
    "    f1m = f1_score(y_test, y_pred_tau, average=\"macro\", zero_division=0)\n",
    "    balc = balanced_accuracy_score(y_test, y_pred_tau)\n",
    "    return f1m, balc\n",
    "\n",
    "taus = [0.55, 0.65, 0.75, 0.8, 0.85]\n",
    "f1_scores, bal_accs = [], []\n",
    "\n",
    "print(\"=== Evaluacion de distintos umbrales tau para 'Undecided' ===\")\n",
    "for tau in taus:\n",
    "    f1m, balc = eval_tau(tau)\n",
    "    f1_scores.append(f1m)\n",
    "    bal_accs.append(balc)\n",
    "    print(f\"tau={tau:.2f} -> Macro F1={f1m:.3f} | Balanced Acc={balc:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([str(t) for t in taus], f1_scores)\n",
    "plt.title(\"Comparacion de Macro F1 segun umbral tau ('Undecided')\")\n",
    "plt.xlabel(\"tau (umbral de Undecided)\")\n",
    "plt.ylabel(\"Macro F1\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccccdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Metrica adicional: Top-2 accuracy (solo decididos) ===\n",
    "top2 = np.argsort(-proba, axis=1)[:, :2]\n",
    "hit_top2_dec = np.mean([\n",
    "    y in top2[i] for i, y in enumerate(y_test)\n",
    "    if label_encoder.classes_[y] != \"Undecided\"\n",
    "])\n",
    "print(f\"Top-2 accuracy (solo decididos): {hit_top2_dec:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce58c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "=== RESUMEN FINAL ===\n",
    "- Modelo base: KNN optimizado con PCA(50), metrica cosine, weights='distance'.\n",
    "- Evaluacion tradicional + post-proceso (umbral tau y Top-2).\n",
    "- Se observa mejora en Macro F1 y Balanced Accuracy tras corregir fuga de validacion y ajustar balanceo.\n",
    "- La grafica muestra el mejor rango de tau (generalmente entre 0.7 y 0.8).\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
